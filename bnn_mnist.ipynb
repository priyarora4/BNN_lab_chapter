{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Neural Networks Demo\n",
        "##### In this project, we develop a binary neural network for MNIST digit classification. Binary neural networks only use -1s or 1s. The main goal of this project is to implement a neural network using XNOR and popcount operators instead of multiplication and summation."
      ],
      "metadata": {
        "id": "0nk1bNrCR5V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "UMAVrTbrWW_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "M6e33CmmWWwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data\n",
        "##### Make sure to add bnn_project.zip to google colab"
      ],
      "metadata": {
        "id": "rHp4OjEMTl8Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PejgMj5oQ8hI"
      },
      "outputs": [],
      "source": [
        "path_to_bnn_zip = \"/content/bnn_project.zip\"\n",
        "with zipfile.ZipFile(path_to_bnn_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(data, true_label, predicated_label):\n",
        "    \"\"\"This function prints image, true label and predicted label.\n",
        "\n",
        "    :param data:\n",
        "    :param true_label:\n",
        "    :param predicated_label:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    plt.imshow(data, cmap='gray')\n",
        "    # Display the image at index 0 from the train_data dataset using imshow()\n",
        "    # The cmap='gray' argument specifies that the image should be displayed in grayscale\n",
        "\n",
        "    plt.title(\"true label: {}, predicted label :{}\".format(true_label, predicated_label))\n",
        "    # Set the title of the plot to the label/target corresponding to the image at index 0\n",
        "    # The '%i' is a placeholder that will be replaced by the value of train_data.targets[0]\n",
        "\n",
        "    plt.show()\n",
        "    # Display the plot\n",
        "\n",
        "\n",
        "# Load the data and visualize\n",
        "\n",
        "mnist = np.load(\"/content/bnn_project/python/dataset/mnist_test_data_original.npy\", allow_pickle=True)\n",
        "X = mnist.item().get(\"data\")\n",
        "y = mnist.item().get(\"label\")\n",
        "print(\"The shape of the input: {}\".format(X.shape))\n",
        "print(\"The shape of the labels: {}\".format(y.shape))\n",
        "print()\n",
        "visualize(X[0], y[0], y[0])\n",
        "\n",
        "\n",
        "X = np.reshape(X, (10000, 784))\n",
        "print()\n",
        "print(\"The shape of the input reshaped: {}\".format(X.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "0dOxHPm0WJII",
        "outputId": "de3c88d6-d411-49af-88eb-546f92bc5721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the input: (10000, 28, 28)\n",
            "The shape of the labels: (10000,)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The shape of the input reshaped: (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal vs XNOR vector multiplication"
      ],
      "metadata": {
        "id": "5CIVA7X4YOj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This is a toy example that demonstrates how to\n",
        "multiply two binary vectors using XNOR and popcount\n",
        "\"\"\"\n",
        "\n",
        "# XNOR operation\n",
        "XNOR = lambda x, y: 1 if x == y else 0\n",
        "\n",
        "# Sign function\n",
        "sign = lambda x: float(1) if x>0 else float(-1)\n",
        "sign = np.vectorize(sign)\n",
        "\n",
        "# Convert -1,1 to 0,1 binary for XNOR operation\n",
        "quantize = lambda x: float(0) if x == 1 else float(1)\n",
        "quantize = np.vectorize(quantize)\n",
        "\n",
        "\n",
        "# Example vectors\n",
        "a = np.array([10, -10, -5, 9, -8, 2, 3, 1, -11])\n",
        "b = np.array([12, -18, -13, -13, -14, -15, 11, 12, 13])\n",
        "\n",
        "# Convert to binary vectors using sign function\n",
        "a = sign(a)\n",
        "b = sign(b)\n",
        "\n",
        "# Normal Matrix multi\n",
        "matmul_result = np.matmul(a, b)\n",
        "\n",
        "# XNOR matmul\n",
        "a = quantize(a)\n",
        "b = quantize(b)\n",
        "\n",
        "xnormatmul_sum = 0\n",
        "for x in range(len(a)):\n",
        "    # Doing XNOR and popcount\n",
        "    xnormatmul_sum = xnormatmul_sum + XNOR(a[x], b[x])\n",
        "\n",
        "\n",
        "\n",
        "# Convince yourself why this next operation is necessary:\n",
        "# In the final XNOR result, say P is the number of 1's (xnormatmul_sum in this case)  and N is the number of 0's\n",
        "# Then in terms of P and N, result = P - N\n",
        "# Notice what we are doing here:\n",
        "# 2*P - len(a) = 2*P - (P+N) = P - N\n",
        "# Which is what we were looking for.\n",
        "xnor_result = 2*xnormatmul_sum-len(a)\n",
        "\n",
        "print(\"Matmul result: {}, XNOR result: {}\".\n",
        "      format(matmul_result, xnor_result))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXr3f8-lXH-r",
        "outputId": "5df19abf-9a38-4811-e1f2-d8b5c30d7ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matmul result: 3.0, XNOR result: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo Walkthrough of BNN\n",
        "### Here is a demo walkthrough of the forward pass of a BNN. We use dummy weights and data inputs"
      ],
      "metadata": {
        "id": "vsznzXoQ26b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy Data\n",
        "X = np.array([1, 1])\n",
        "\n",
        "w1 = np.array([[1, 1],\n",
        "               [1, 1]])\n",
        "\n",
        "w2 = np.array([[1, 0, 1],\n",
        "               [1, 1, 0]])\n",
        "# w3 = np.array([[1, 0, 1],\n",
        "#                [0, 1]])"
      ],
      "metadata": {
        "id": "5P0q1Fqs4NA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining operation functions\n",
        "\n",
        "def XNOR(a, b):\n",
        "    return 1 if a == b else 0\n",
        "\n",
        "def matmul_xnor(A, B):\n",
        "    a, b = B.shape\n",
        "    res = np.zeros(b)\n",
        "\n",
        "    A1 = A.astype(int)\n",
        "    B1 = B.astype(int)\n",
        "\n",
        "    for x in range(b):\n",
        "        cnt = 0\n",
        "        for y in range(a):\n",
        "            cnt = cnt + XNOR(A1[y], B1[y][x])\n",
        "\n",
        "        res[x] = cnt\n",
        "    return res\n",
        "\n",
        "def quantize(x):\n",
        "  return float(0) if x == 1 else float(1)\n",
        "\n",
        "def sign(x):\n",
        "  return float(1) if x > 0 else float(-1)\n",
        "\n",
        "def adj(x):\n",
        "  return x * 2 - 1"
      ],
      "metadata": {
        "id": "mQFN3v5A4RfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "# Notice that we do not vectorize any of the operations, rather we use full for loops.\n",
        "# Later in this notebook you will notice that we vectorize these operations.\n",
        "\n",
        "def feed_forward_quantized(X, w1, w2):\n",
        "\n",
        "    # Layer 1\n",
        "    X0_input = np.zeros(X.shape)\n",
        "    for i in range(X.shape[0]):\n",
        "        X0_input[i] = quantize(sign(adj(X[i])))\n",
        "\n",
        "    layer1_output = matmul_xnor(X0_input, w1)\n",
        "    layer1_activations = (layer1_output * 2 - X0_input.shape[0])\n",
        "\n",
        "    # Layer 2\n",
        "    layer2_quantized = np.zeros(layer1_activations.shape)\n",
        "    for i in range(layer1_activations.shape[0]):\n",
        "        layer2_quantized[i] = quantize(sign(layer1_activations[i]))\n",
        "    layer2_output = matmul_xnor(layer2_quantized, w2)\n",
        "    final_output = (layer2_output * 2 - layer1_activations.shape[0])\n",
        "    A = np.array([final_output], np.int32)\n",
        "    return A\n",
        "\n",
        "print(feed_forward_quantized(X, w1, w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxnFZ5fA252h",
        "outputId": "d84c8b11-b929-46bb-eee0-97949c40068c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BNN Model Class\n",
        "#### Next we will make predictions on the actual MNIST dataset."
      ],
      "metadata": {
        "id": "py1A72Jyg8ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BNN_MNIST:\n",
        "\n",
        "    def __init__(self, model_path, batch_size=64):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Sign function for converting to +/- 1\n",
        "        self.sign = lambda x: float(1) if x>0 else float(-1)\n",
        "        self.sign = np.vectorize(self.sign)\n",
        "\n",
        "        # Quantize function for representing +/- 1 as 0 or 1.\n",
        "        self.quantize = lambda x: float(0) if x == 1 else float(1)\n",
        "        self.quantize = np.vectorize(self.quantize)\n",
        "\n",
        "        # Adjust input pixel intensities\n",
        "        self.adj = lambda x: x*2-1\n",
        "        self.adj = np.vectorize(self.adj)\n",
        "\n",
        "        # Load model\n",
        "        self.model = np.load(model_path, allow_pickle=True).item()\n",
        "\n",
        "        # Layer 1 weights (128, 784)\n",
        "        self.fc1w_q = self.sign(np.array(self.model['fc1w']))\n",
        "        # Layer 2 weights (64, 128)\n",
        "        self.fc2w_q = self.sign(np.array(self.model['fc2w']))\n",
        "        # Layer3 weights (64, 10)\n",
        "        self.fc3w_q = self.sign(np.array(self.model['fc3w']))\n",
        "\n",
        "        # Quantisized weights (1s and 0s)\n",
        "        self.fc1w_qntz = self.quantize(self.fc1w_q)\n",
        "        self.fc2w_qntz = self.quantize(self.fc2w_q)\n",
        "        self.fc3w_qntz = self.quantize(self.fc3w_q)\n",
        "\n",
        "    def feed_forward(self, input):\n",
        "        \"\"\"This BNN using normal MAC.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        X0_q = self.sign(self.adj(input))\n",
        "\n",
        "        X1 = np.matmul(X0_q, self.fc1w_q.T)\n",
        "\n",
        "        X1_q = self.sign(X1)\n",
        "\n",
        "        X2 = np.matmul(X1_q, self.fc2w_q.T)\n",
        "\n",
        "        X2_q = self.sign(X2)\n",
        "\n",
        "        X3 = np.matmul(X2_q, self.fc3w_q.T)\n",
        "\n",
        "        return X3\n",
        "\n",
        "    def feed_forward_quantized(self, input):\n",
        "        \"\"\"This function does BNN. Uses XNOR. Note this function only works for batch size 1.\n",
        "\n",
        "        :param input: MNIST sample input\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # layer 1\n",
        "        X0_input = self.quantize(self.sign(self.adj(input)))[0]\n",
        "        layer1_output = self.matmul_xnor(X0_input, self.fc1w_qntz.T)\n",
        "        # Popcount for each activation. Look back to \"Normal vs XNOR vector multiplication\"\n",
        "        # to see why this is necessary\n",
        "        layer1_activations = (layer1_output * 2 - 784)\n",
        "\n",
        "\n",
        "        # layer 2\n",
        "        layer2_input = self.sign(layer1_activations)\n",
        "        layer2_quantized = self.quantize(layer2_input)\n",
        "        layer2_output = self.matmul_xnor(layer2_quantized, self.fc2w_qntz.T)\n",
        "        layer2_activations = (layer2_output * 2 - 128)\n",
        "\n",
        "\n",
        "        # layer 3\n",
        "        layer3_input = self.sign(layer2_activations)\n",
        "        layer3_quantized = self.quantize(layer3_input)\n",
        "        layer3_output = self.matmul_xnor(layer3_quantized, self.fc3w_qntz.T)\n",
        "\n",
        "        final_output = (layer3_output * 2 - 64)\n",
        "        A = np.array([final_output], np.int32)\n",
        "\n",
        "        return A\n",
        "\n",
        "    def XNOR(self, a, b):\n",
        "        if (a == b):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def matmul_xnor(self, A, B):\n",
        "        \"\"\"This function calculates matrix multiplication between two vectors using XNOR.\n",
        "        Currently only functional for batch size of 1.\n",
        "\n",
        "        :param A: The first quantized vector\n",
        "        :param B: The second quantized  vector\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        a, b = B.shape\n",
        "\n",
        "        res = np.zeros(b)\n",
        "\n",
        "        A1 = A.astype(int)\n",
        "        B1 = B.astype(int)\n",
        "\n",
        "        for x in range(b):\n",
        "            cnt = 0\n",
        "            for y in range(a):\n",
        "              cnt = cnt + self.XNOR(A1[y], B1[y][x])\n",
        "\n",
        "            res[x] = cnt\n",
        "        return res\n",
        "\n",
        "\n",
        "    def pack(self, A, n):\n",
        "        \"\"\"Helper function for converting numpy array of bits to 32 bit integer array.\n",
        "        :param A: numpy array\n",
        "        :param n: A.shape[0]*A.shape[1] i.e. total number of bits in array.\n",
        "        :return:\n",
        "\n",
        "        \"\"\"\n",
        "        A_bit = np.array([0] * (n // 32), dtype=np.uint32)\n",
        "\n",
        "        A_lin = np.reshape(A, (n,))\n",
        "\n",
        "        # convert every 32 bit sequence into a single integer number.\n",
        "        for i in range(0, n, 32):\n",
        "            A_bit[i // 32] = self.concat4(A_lin, i)\n",
        "\n",
        "        return A_bit\n",
        "\n",
        "\n",
        "    def quantize_scale(self, x):\n",
        "        \"\"\"Helper function for converting +/- 1 to 1s and 0s\n",
        "        :param x:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        if x == -1:\n",
        "            return 1\n",
        "        elif x == 1:\n",
        "            return 0\n",
        "\n",
        "    def concat4(self, li, point):\n",
        "        \"\"\"Helper function for converting the bits li[point: point+32] to a single interger.\n",
        "        :param li: Array\n",
        "        :param point: integer index into the array\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        result = np.array([self.quantize_scale(li[point])], dtype=np.uint32)[0].astype(np.uint32)\n",
        "\n",
        "\n",
        "        for k in range(1, 32):\n",
        "            i = point + k\n",
        "            result <<= 1\n",
        "            result &= 0xFFFFFFFF\n",
        "            result |= self.quantize_scale(li[i])\n",
        "            result &= 0xFFFFFFFF\n",
        "        return result.astype(np.uint32)\n",
        "\n",
        "    def preprocessModel(self, dataset_folder_path, X, y):\n",
        "        \"\"\"Helper function to convert input data, X, to 32 bit integers and save them along with the labels y.\n",
        "        :param X:\n",
        "        :param y:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        sample = 2\n",
        "        numpydict = {\"X\": [], \"y\": []}\n",
        "\n",
        "        #  X is shape (N x 784)\n",
        "        # Pad each sample with 16 1s so that X is (N x 800)\n",
        "        # We do this because when converting the bit array to integers, we need\n",
        "        # the array size to be a multiple of 32. 800 is a multiple of 32 however, 784 is not.\n",
        "        # Therefore we pad.\n",
        "        X0_q = np.array([list(arr) + [1] * 16 for arr in X])\n",
        "\n",
        "        # See documentation for function pack()\n",
        "        X0_bit = self.pack(X0_q, X0_q.shape[0] * X0_q.shape[1])\n",
        "        Y0 = y\n",
        "        numpydict[\"X\"].append(X0_bit)\n",
        "        numpydict[\"y\"].append(Y0)\n",
        "\n",
        "        np.save(f'{dataset_folder_path}/mnist-bit_sample{sample}.npy', numpydict, allow_pickle=True)\n",
        "\n",
        "\n",
        "    def create_input(self, data_path, dataset_folder_path, num_of_samples):\n",
        "        \"\"\"This function creates packed inputs for a given number of samples.\n",
        "        The created inputs are used for HLS tesbench. \"Packed\" means to convert\n",
        "        the input array into 32 bit integer numbers.\n",
        "\n",
        "        :param num_of_samples:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        mnist = np.load(data_path, allow_pickle=True)\n",
        "        X = mnist.item().get(\"data\")\n",
        "        y = mnist.item().get(\"label\")\n",
        "\n",
        "        X = np.reshape(X, (10000, 784))\n",
        "\n",
        "        if num_of_samples == 0:\n",
        "            num_of_samples = (len(X) // self.batch_size)\n",
        "\n",
        "        numpydict = {\"X\": [], \"y\": []}\n",
        "\n",
        "        for idx in range(num_of_samples):\n",
        "            # Get batch\n",
        "            xs = X[self.batch_size * idx:self.batch_size * idx + self.batch_size]\n",
        "            ys = y[self.batch_size * idx:self.batch_size * idx + self.batch_size]\n",
        "\n",
        "            X0_q = self.sign(self.adj(xs))\n",
        "            self.preprocessModel(dataset_folder_path, X0_q, ys)\n",
        "\n",
        "            #  X is shape (N x 784)\n",
        "            # Pad each sample with 16 1s so that X is (N x 800)\n",
        "            # We do this because when converting the bit array to integers, we need\n",
        "            # the array size to be a multiple of 32. 800 is a multiple of 32 however, 784 is not.\n",
        "            # Therefore we pad.\n",
        "            X0_q = np.array([list(arr) + [1] * 16 for arr in X0_q])\n",
        "\n",
        "            # See function pack()\n",
        "            X0_bit = self.pack(X0_q, X0_q.shape[0] * X0_q.shape[1])\n",
        "            Y0 = ys\n",
        "            numpydict[\"X\"].append(X0_bit)\n",
        "            numpydict[\"y\"].append(Y0)\n",
        "\n",
        "        np.save(f'{dataset_folder_path}/mnist-bit_sample{num_of_samples}.npy', numpydict, allow_pickle=True)\n",
        "\n",
        "\n",
        "\n",
        "    def create_packed_weights(self, weights_folder_path):\n",
        "        \"\"\"Helper function: This function creates packed weights for HLS.\n",
        "        \"Packed\" means to convert the weights arrays into 32 bit integer numbers.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # fc1w_q is shape (128 x 784)\n",
        "        # Pad each row with 16 1s so that fc1w_q is (128 x 800)\n",
        "        # We do this because when converting the bit array to integers, we need\n",
        "        # the array size to be a multiple of 32. 800 is a multiple of 32 however, 784 is not.\n",
        "        # Therefore we pad.\n",
        "        fc1w_q = np.array([list(arr) + ([1] * 16) for arr in self.fc1w_q])\n",
        "        # See function pack()\n",
        "        fc1w_bit = self.pack(fc1w_q.T.T, fc1w_q.shape[0] * fc1w_q.shape[1])\n",
        "        fc1w_bit = self.pack(fc1w_q, fc1w_q.shape[0] * fc1w_q.shape[1])\n",
        "\n",
        "        fc2w_q = self.fc2w_q\n",
        "        fc2w_bit = self.pack(fc2w_q, fc2w_q.shape[0] * fc2w_q.shape[1])\n",
        "\n",
        "        fc3w_q = self.fc3w_q\n",
        "        fc3w_bit = self.pack(fc3w_q, fc3w_q.shape[0] * fc3w_q.shape[1])\n",
        "\n",
        "        np.savetxt(f'{weights_folder_path}/layer1.txt', fc1w_bit)\n",
        "        np.savetxt(f'{weights_folder_path}/layer2.txt', fc2w_bit)\n",
        "        np.savetxt(f'{weights_folder_path}/layer3.txt', fc3w_bit)\n",
        "\n",
        "\n",
        "        return fc1w_bit, fc2w_bit, fc3w_bit\n",
        "\n",
        "\n",
        "    def write_to_file(self, weights_folder_path, w1, w2, w3):\n",
        "        \"\"\" Helper function\n",
        "        :param w1:\n",
        "        :param w2:\n",
        "        :param w3:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Writing to file\n",
        "        with open(f\"{weights_folder_path}/layer1_c.txt\", \"w\") as file1:\n",
        "            # Writing data to a file\n",
        "            for x in range(len(w1)):\n",
        "                file1.writelines(str(w1[x])+\",\")\n",
        "\n",
        "        with open(f\"{weights_folder_path}/layer2_c.txt\", \"w\") as file1:\n",
        "            # Writing data to a file\n",
        "            for x in range(len(w2)):\n",
        "                file1.writelines(str(w2[x])+\",\")\n",
        "\n",
        "        with open(f\"{weights_folder_path}/layer3_c.txt\", \"w\") as file1:\n",
        "            # Writing data to a file\n",
        "            for x in range(len(w3)):\n",
        "                file1.writelines(str(w3[x])+\",\")\n",
        "\n",
        "    def hlscode(self):\n",
        "        \"\"\"This is a reference implementation for HLS.\n",
        "        Intentionally, left empty so that students implement the HLS ref design.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Done\")\n",
        "\n",
        "\n",
        "    def visualize(self, data, true_label, predicated_label):\n",
        "        \"\"\"This function prints image, true label and predicted label.\n",
        "\n",
        "        :param data:\n",
        "        :param true_label:\n",
        "        :param predicated_label:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Visualization\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Import the matplotlib.pyplot module for visualization purposes\n",
        "\n",
        "        plt.imshow(data, cmap='gray')\n",
        "        # Display the image at index 0 from the train_data dataset using imshow()\n",
        "        # The cmap='gray' argument specifies that the image should be displayed in grayscale\n",
        "\n",
        "        plt.title(\"true label: {}, predicted label :{}\".format(true_label, predicated_label))\n",
        "        # Set the title of the plot to the label/target corresponding to the image at index 0\n",
        "        # The '%i' is a placeholder that will be replaced by the value of train_data.targets[0]\n",
        "\n",
        "        plt.show()\n",
        "        # Display the plot\n",
        "\n",
        "    def run_test_visalize(self, data_path, num_samples):\n",
        "        \"\"\"This function is for debugging. Used for visualizing\n",
        "        the input (MNIST image), the predicted output and the true output.\n",
        "        :param data_path: path to the mnist dataset\n",
        "        :param num_samples: number of samples to visualize\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        mnist = np.load(data_path, allow_pickle=True)\n",
        "        X = mnist.item().get(\"data\")\n",
        "        y = mnist.item().get(\"label\")\n",
        "\n",
        "        X = np.reshape(X, (10000, 784))\n",
        "        print(X.shape)\n",
        "\n",
        "\n",
        "        # Visulize input image prediction and true label for all samples\n",
        "        for idx in range(num_samples):\n",
        "            xs = X[idx]\n",
        "            ys = y[idx]\n",
        "            outputs = self.feed_forward(xs)\n",
        "            xs_plot = np.reshape(xs, (28, 28))\n",
        "            self.visualize(xs_plot, ys, np.argmax(outputs))\n",
        "\n",
        "    def run_test(self, X, y, use_normal_mac=False):\n",
        "        \"\"\"This function is for testing\n",
        "\n",
        "        :param use_normal_mac: Setting this parameter calls self.feed_forward (uses MAC),\n",
        "        otherwise, it calls feed_forward_quantized which uses XNOR\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        prediction = []\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        # set inference function\n",
        "        if use_normal_mac is True:\n",
        "            inference_function = self.feed_forward\n",
        "        else:\n",
        "            inference_function = self.feed_forward_quantized\n",
        "\n",
        "\n",
        "        for idx in tqdm(range(len(X) // self.batch_size)):\n",
        "            # Get Batch\n",
        "            xs = X[self.batch_size * idx:self.batch_size * idx + self.batch_size]\n",
        "            ys = y[self.batch_size * idx:self.batch_size * idx + self.batch_size]\n",
        "\n",
        "            # Inference\n",
        "            outputs = inference_function(xs)\n",
        "\n",
        "            # Prediction correct or not\n",
        "            for output, yk in zip(outputs, ys):\n",
        "                prediction.append(np.argmax(output) == (yk))\n",
        "            i += 1\n",
        "\n",
        "\n",
        "        # calculate score\n",
        "        score = np.mean(prediction) * 100\n",
        "\n",
        "        return score\n",
        "\n"
      ],
      "metadata": {
        "id": "7tTjJK4AkePk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference on MNIST\n"
      ],
      "metadata": {
        "id": "QJPWwQkH2LQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure data is loaded\n",
        "print(\"The shape of the input: {}\".format(X.shape))\n",
        "print(\"The shape of the output: {}\".format(y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dICAMJMK3H1R",
        "outputId": "51b88326-4633-4362-fa65-6139abb04efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the input: (2,)\n",
            "The shape of the output: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model Instance\n",
        "# Note that the XNOR matrix multiplication implimentation only works with batch_size 1 currently.\n",
        "bnn = BNN_MNIST(model_path=\"/content/bnn_project/python/weights/model.npy\", batch_size=1)"
      ],
      "metadata": {
        "id": "09896keOmmzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Model Using normal matrix multiplication\n",
        "print(\"Running BNN which uses MAC\")\n",
        "print(\"\\nAccuracy:\", bnn.run_test(X, y, use_normal_mac=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF7fviIX6uxb",
        "outputId": "ec824973-60fa-4599-e723-82b6d1df8eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running BNN which uses MAC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:24<00:00, 409.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 89.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Model using XNOR for matrix multiplication\n",
        "# Note that this may take up to 15 minutes\n",
        "print(\"Running BNN which uses XNOR\")\n",
        "print(\"Accuracy:\", bnn.run_test(X[:10], y, use_normal_mac=False))\n"
      ],
      "metadata": {
        "id": "C2Cd68HA2JW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f970035-ab9a-4c1c-85d2-ce8ad13312a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running BNN which uses XNOR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 12.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 90.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO In the BNN class, the function hls() is for you to complete"
      ],
      "metadata": {
        "id": "0od9TOKS6OlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnn.hlscode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7CtS2FKS0LW",
        "outputId": "5d896006-a25c-42d0-c064-00bea2dbfb39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1WvScxy6tGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}